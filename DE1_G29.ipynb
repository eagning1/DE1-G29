{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/eagning1/DE1-G29/blob/main/DE1_G29.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Rzj3uCh8tcHJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import warnings\n",
    "from timeit import default_timer as timer\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\erkoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download VADER lexicon if not already downloaded\n",
    "try:\n",
    "    nltk.data.find('vader_lexicon')\n",
    "except LookupError:\n",
    "    nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for our plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsAnalyzer:\n",
    "    def __init__(self, matches_file, train_file):\n",
    "        \"\"\"\n",
    "        Initialize the LyricsAnalyzer with the paths to the dataset files.\n",
    "        Parameters:\n",
    "        -----------\n",
    "        matches_file : str\n",
    "            Path to the mxm_779k_matches.txt file\n",
    "        train_file : str\n",
    "            Path to the mxm_dataset_train.txt file\n",
    "        \"\"\"\n",
    "        self.matches_file = matches_file\n",
    "        self.train_file = train_file\n",
    "        self.top_words = []\n",
    "        self.matches_df = None\n",
    "        self.lyrics_df = None\n",
    "        self.word_counts = None\n",
    "        self.sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "        \n",
    "    def load_matches(self):\n",
    "        \"\"\"Load and parse the matches file\"\"\"\n",
    "        print(\"Loading matches data...\")\n",
    "        # Define column names based on the file format\n",
    "        columns = ['msd_track_id', 'msd_artist_name', 'msd_title', 'mxm_track_id', 'mxm_artist_name', 'mxm_title']\n",
    "        \n",
    "        # Read the file\n",
    "        data = []\n",
    "        with open(self.matches_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('#'):  # Skip comment lines\n",
    "                    continue\n",
    "                parts = line.strip().split('<SEP>')\n",
    "                if len(parts) == 6:  # Ensure we have all fields\n",
    "                    data.append(parts)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        self.matches_df = pd.DataFrame(data, columns=columns)\n",
    "        print(f\"Loaded {len(self.matches_df)} song matches.\")\n",
    "        return self.matches_df\n",
    "    \n",
    "    def load_lyrics(self):\n",
    "        \"\"\"Load and parse the lyrics training file\"\"\"\n",
    "        print(\"Loading lyrics data...\")\n",
    "        # First, extract the top words list\n",
    "        self.top_words = []\n",
    "        word_counts_data = []\n",
    "        \n",
    "        with open(self.train_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('#'):  # Skip comment lines\n",
    "                    continue\n",
    "                elif line.startswith('%'):  # Extract top words\n",
    "                    self.top_words = line[1:].strip().split(',')\n",
    "                else:\n",
    "                    # Parse the word count line\n",
    "                    parts = line.strip().split(',')\n",
    "                    if len(parts) >= 2:\n",
    "                        track_id = parts[0]\n",
    "                        mxm_track_id = parts[1]\n",
    "                        \n",
    "                        # Create a sparse word count dictionary\n",
    "                        word_counts = {}\n",
    "                        for item in parts[2:]:\n",
    "                            if ':' in item:\n",
    "                                idx, count = item.split(':')\n",
    "                                # Convert to 0-based index and ensure it's within range\n",
    "                                word_idx = int(idx) - 1  # 1-based to 0-based\n",
    "                                if word_idx < len(self.top_words):\n",
    "                                    word_counts[self.top_words[word_idx]] = int(count)\n",
    "                        \n",
    "                        word_counts_data.append({\n",
    "                            'track_id': track_id,\n",
    "                            'mxm_track_id': mxm_track_id,\n",
    "                            'word_counts': word_counts\n",
    "                        })\n",
    "        \n",
    "        # Create DataFrame\n",
    "        self.lyrics_df = pd.DataFrame(word_counts_data)\n",
    "        print(f\"Loaded lyrics data for {len(self.lyrics_df)} songs with {len(self.top_words)} vocabulary words.\")\n",
    "        return self.lyrics_df\n",
    "    \n",
    "    def merge_data(self):\n",
    "        \"\"\"Merge the matches and lyrics data\"\"\"\n",
    "        if self.matches_df is None:\n",
    "            self.load_matches()\n",
    "        if self.lyrics_df is None:\n",
    "            self.load_lyrics()\n",
    "        \n",
    "        # Merge on mxm_track_id\n",
    "        merged_df = pd.merge(\n",
    "            self.lyrics_df,\n",
    "            self.matches_df,\n",
    "            left_on='mxm_track_id',\n",
    "            right_on='mxm_track_id',\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        print(f\"Merged data contains {len(merged_df)} songs.\")\n",
    "        return merged_df\n",
    "    \n",
    "    def create_word_count_matrix(self):\n",
    "        \"\"\"Convert sparse word counts to a matrix format\"\"\"\n",
    "        if self.lyrics_df is None:\n",
    "            self.load_lyrics()\n",
    "        \n",
    "        # Create a matrix where rows are songs and columns are words\n",
    "        word_count_matrix = np.zeros((len(self.lyrics_df), len(self.top_words)))\n",
    "        \n",
    "        for i, row in enumerate(self.lyrics_df['word_counts']):\n",
    "            for word, count in row.items():\n",
    "                if word in self.top_words:\n",
    "                    col_idx = self.top_words.index(word)\n",
    "                    word_count_matrix[i, col_idx] = count\n",
    "        \n",
    "        self.word_counts = pd.DataFrame(word_count_matrix, columns=self.top_words)\n",
    "        return self.word_counts\n",
    "    \n",
    "    def calculate_sentiment(self, merged_data=None):\n",
    "        \"\"\"Calculate sentiment scores for each song based on its word counts\"\"\"\n",
    "        if merged_data is None:\n",
    "            merged_data = self.merge_data()\n",
    "        \n",
    "        sentiment_scores = []\n",
    "        \n",
    "        for _, row in merged_data.iterrows():\n",
    "            word_counts = row['word_counts']\n",
    "            \n",
    "            # Convert word counts to a pseudo-text for VADER\n",
    "            # Repeat each word by its count to give it proper weight\n",
    "            pseudo_text = ' '.join([f\"{word} \" * count for word, count in word_counts.items()])\n",
    "            \n",
    "            # Get sentiment scores\n",
    "            sentiment = self.sentiment_analyzer.polarity_scores(pseudo_text)\n",
    "            sentiment_scores.append({\n",
    "                'track_id': row['track_id'],\n",
    "                'artist': row['msd_artist_name'],\n",
    "                'title': row['msd_title'],\n",
    "                'negative': sentiment['neg'],\n",
    "                'neutral': sentiment['neu'],\n",
    "                'positive': sentiment['pos'],\n",
    "                'compound': sentiment['compound']\n",
    "            })\n",
    "        \n",
    "        sentiment_df = pd.DataFrame(sentiment_scores)\n",
    "        return sentiment_df\n",
    "\n",
    "    def load_genre_data(self, genre_file):\n",
    "        \"\"\"Load genre assignments for tracks\"\"\"\n",
    "        print(\"Loading genre data...\")\n",
    "        genre_data = {}\n",
    "        \n",
    "        with open(genre_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('#'):  # Skip comment lines\n",
    "                    continue\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    track_id, genre = parts\n",
    "                    genre_data[track_id] = genre\n",
    "        \n",
    "        print(f\"Loaded genre data for {len(genre_data)} songs.\")\n",
    "        return genre_data\n",
    "    \n",
    "    def merge_genre_data(self, genre_data, merged_data=None):\n",
    "        \"\"\"Add genre information to the merged dataset\"\"\"\n",
    "        if merged_data is None:\n",
    "            merged_data = self.merge_data()\n",
    "        \n",
    "        # Create a new column for genre\n",
    "        merged_data['genre'] = merged_data['track_id'].map(genre_data)\n",
    "        \n",
    "        # Filter out tracks without genre information\n",
    "        merged_data_with_genre = merged_data.dropna(subset=['genre'])\n",
    "        print(f\"Merged data contains {len(merged_data_with_genre)} songs with genre information.\")\n",
    "        \n",
    "        return merged_data_with_genre\n",
    "    \n",
    "    def prepare_features_for_classification(self, data=None, remove_words=None):\n",
    "        \"\"\"Prepare features for genre classification, optionally removing specific words\"\"\"\n",
    "        if data is None:\n",
    "            data = self.merge_data()\n",
    "            \n",
    "        if remove_words is None:\n",
    "            remove_words = ['i', 'the', 'a', 'to', 'you', 'and', 'me']\n",
    "        \n",
    "        # Create features from word counts\n",
    "        features = []\n",
    "        \n",
    "        for _, row in data.iterrows():\n",
    "            word_counts = row['word_counts']\n",
    "            # Remove specified words\n",
    "            filtered_counts = {word: count for word, count in word_counts.items() \n",
    "                             if word not in remove_words}\n",
    "            features.append(filtered_counts)\n",
    "        \n",
    "        # Convert to matrix format\n",
    "        from sklearn.feature_extraction import DictVectorizer\n",
    "        vectorizer = DictVectorizer(sparse=True)\n",
    "        X = vectorizer.fit_transform(features)\n",
    "        \n",
    "        return X, vectorizer\n",
    "    \n",
    "    def train_genre_classifier(self, X_train, y_train):\n",
    "        \"\"\"Train an SVM classifier for genre prediction\"\"\"\n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        \n",
    "        # Create a pipeline with scaling and SVM\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler(with_mean=False)),  # Don't subtract mean for sparse data\n",
    "            ('svm', SVC(kernel='linear', C=1.0, random_state=42))\n",
    "        ])\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"Training SVM classifier for genre prediction...\")\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        return pipeline\n",
    "    \n",
    "    def evaluate_genre_classifier(self, model, X_test, y_test):\n",
    "        \"\"\"Evaluate the genre classifier\"\"\"\n",
    "        from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "        import seaborn as sns\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Classification report\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(report)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=sorted(set(y_test)),\n",
    "                   yticklabels=sorted(set(y_test)))\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        \n",
    "        return accuracy, report, plt\n",
    "    \n",
    "    def load_test_data(self, test_file):\n",
    "        \"\"\"Load and parse the test dataset\"\"\"\n",
    "        print(\"Loading test data...\")\n",
    "        test_words = []\n",
    "        test_data = []\n",
    "        \n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('#'):  # Skip comment lines\n",
    "                    continue\n",
    "                elif line.startswith('%'):  # Extract words list\n",
    "                    test_words = line[1:].strip().split(',')\n",
    "                else:\n",
    "                    # Parse the word count line\n",
    "                    parts = line.strip().split(',')\n",
    "                    if len(parts) >= 2:\n",
    "                        track_id = parts[0]\n",
    "                        mxm_track_id = parts[1]\n",
    "                        \n",
    "                        # Create a sparse word count dictionary\n",
    "                        word_counts = {}\n",
    "                        for item in parts[2:]:\n",
    "                            if ':' in item:\n",
    "                                idx, count = item.split(':')\n",
    "                                # Convert to 0-based index and ensure it's within range\n",
    "                                word_idx = int(idx) - 1  # 1-based to 0-based\n",
    "                                if word_idx < len(test_words):\n",
    "                                    word_counts[test_words[word_idx]] = int(count)\n",
    "                        \n",
    "                        test_data.append({\n",
    "                            'track_id': track_id,\n",
    "                            'mxm_track_id': mxm_track_id,\n",
    "                            'word_counts': word_counts\n",
    "                        })\n",
    "        \n",
    "        test_df = pd.DataFrame(test_data)\n",
    "        print(f\"Loaded test data for {len(test_df)} songs with {len(test_words)} vocabulary words.\")\n",
    "        \n",
    "        # Ensure test_words match training words by appending any missing ones\n",
    "        if len(test_words) != len(self.top_words) or test_words != self.top_words:\n",
    "            print(\"Warning: Test vocabulary differs from training vocabulary.\")\n",
    "            \n",
    "        return test_df, test_words\n",
    "    \n",
    "    def predict_genres(self, model, vectorizer, test_df, genre_data=None, remove_words=None):\n",
    "        \"\"\"Predict genres for the test dataset\"\"\"\n",
    "        if remove_words is None:\n",
    "            remove_words = ['i', 'the', 'a', 'to', 'you', 'and', 'me']\n",
    "        \n",
    "        # Prepare test features\n",
    "        test_features = []\n",
    "        for _, row in test_df.iterrows():\n",
    "            word_counts = row['word_counts']\n",
    "            # Remove specified words\n",
    "            filtered_counts = {word: count for word, count in word_counts.items() \n",
    "                             if word not in remove_words}\n",
    "            test_features.append(filtered_counts)\n",
    "        \n",
    "        # Transform using the same vectorizer as training\n",
    "        X_test = vectorizer.transform(test_features)\n",
    "        \n",
    "        # Make predictions\n",
    "        print(\"Predicting genres for test data...\")\n",
    "        predicted_genres = model.predict(X_test)\n",
    "        \n",
    "        # Add predictions to test data\n",
    "        predictions_df = test_df.copy()\n",
    "        predictions_df['predicted_genre'] = predicted_genres\n",
    "        \n",
    "        # If ground truth is available, add it\n",
    "        if genre_data is not None:\n",
    "            predictions_df['actual_genre'] = predictions_df['track_id'].map(genre_data)\n",
    "            \n",
    "            # Calculate accuracy for test data with known genres\n",
    "            mask = ~predictions_df['actual_genre'].isna()\n",
    "            if mask.sum() > 0:\n",
    "                accuracy = (predictions_df.loc[mask, 'predicted_genre'] == \n",
    "                           predictions_df.loc[mask, 'actual_genre']).mean()\n",
    "                print(f\"Test accuracy: {accuracy:.4f} (for {mask.sum()} songs with known genres)\")\n",
    "        \n",
    "        return predictions_df\n",
    "    \n",
    "    def reduce_dimensions(self, method='pca', n_components=2):\n",
    "        \"\"\"Reduce dimensions of word count data for visualization\"\"\"\n",
    "        if self.word_counts is None:\n",
    "            self.create_word_count_matrix()\n",
    "        \n",
    "        # Normalize word counts\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(self.word_counts)\n",
    "        \n",
    "        if method.lower() == 'pca':\n",
    "            reducer = PCA(n_components=n_components)\n",
    "        elif method.lower() == 'tsne':\n",
    "            reducer = TSNE(n_components=n_components, random_state=42)\n",
    "        else:\n",
    "            raise ValueError(\"Method must be either 'pca' or 'tsne'\")\n",
    "        \n",
    "        reduced_features = reducer.fit_transform(scaled_features)\n",
    "        \n",
    "        return reduced_features\n",
    "    \n",
    "    def plot_most_common_words(self, top_n=20):\n",
    "        \"\"\"Plot the most common words across all songs\"\"\"\n",
    "        if self.word_counts is None:\n",
    "            self.create_word_count_matrix()\n",
    "        \n",
    "        # Sum word counts across all songs\n",
    "        total_counts = self.word_counts.sum().sort_values(ascending=False)\n",
    "        top_words = total_counts.head(top_n)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x=top_words.values, y=top_words.index)\n",
    "        plt.title(f'Top {top_n} Most Common Words in Lyrics')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('Word')\n",
    "        plt.tight_layout()\n",
    "        return plt\n",
    "    \n",
    "    def plot_sentiment_distribution(self, sentiment_df=None):\n",
    "        \"\"\"Plot the distribution of sentiment scores\"\"\"\n",
    "        if sentiment_df is None:\n",
    "            sentiment_df = self.calculate_sentiment()\n",
    "        \n",
    "        fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Plot distributions of each sentiment score\n",
    "        sns.histplot(sentiment_df['negative'], kde=True, ax=axs[0, 0], color='red')\n",
    "        axs[0, 0].set_title('Negative Sentiment Distribution')\n",
    "        \n",
    "        sns.histplot(sentiment_df['neutral'], kde=True, ax=axs[0, 1], color='gray')\n",
    "        axs[0, 1].set_title('Neutral Sentiment Distribution')\n",
    "        \n",
    "        sns.histplot(sentiment_df['positive'], kde=True, ax=axs[1, 0], color='green')\n",
    "        axs[1, 0].set_title('Positive Sentiment Distribution')\n",
    "        \n",
    "        sns.histplot(sentiment_df['compound'], kde=True, ax=axs[1, 1], color='blue')\n",
    "        axs[1, 1].set_title('Compound Sentiment Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig, axs\n",
    "    \n",
    "    def analyze_artist(self, artist_name):\n",
    "        \"\"\"Analyze lyrics patterns for a specific artist\"\"\"\n",
    "        merged_data = self.merge_data()\n",
    "        \n",
    "        # Filter for the artist\n",
    "        artist_data = merged_data[merged_data['msd_artist_name'].str.lower() == artist_name.lower()]\n",
    "        \n",
    "        if len(artist_data) == 0:\n",
    "            print(f\"No data found for artist: {artist_name}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Analyzing {len(artist_data)} songs by {artist_name}\")\n",
    "        \n",
    "        # Calculate sentiment for the artist's songs\n",
    "        artist_sentiment = self.calculate_sentiment(artist_data)\n",
    "        \n",
    "        # Get most common words for this artist\n",
    "        all_words = Counter()\n",
    "        for word_counts in artist_data['word_counts']:\n",
    "            all_words.update(word_counts)\n",
    "        \n",
    "        return {\n",
    "            'artist_data': artist_data,\n",
    "            'sentiment': artist_sentiment,\n",
    "            'common_words': all_words\n",
    "        }\n",
    "    \n",
    "    def compare_genres(self, genre_artist_mapping):\n",
    "        \"\"\"\n",
    "        Compare lyrics and sentiment patterns across different genres\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        genre_artist_mapping : dict\n",
    "            Dictionary mapping genre names to lists of artists in that genre\n",
    "        \"\"\"\n",
    "        merged_data = self.merge_data()\n",
    "        genre_stats = {}\n",
    "        \n",
    "        for genre, artists in genre_artist_mapping.items():\n",
    "            # Get data for all artists in this genre\n",
    "            genre_data = merged_data[merged_data['msd_artist_name'].str.lower().isin([a.lower() for a in artists])]\n",
    "            \n",
    "            if len(genre_data) == 0:\n",
    "                print(f\"No data found for genre: {genre}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Analyzing {len(genre_data)} songs in the {genre} genre\")\n",
    "            \n",
    "            # Calculate sentiment \n",
    "            genre_sentiment = self.calculate_sentiment(genre_data)\n",
    "            \n",
    "            # Get most common words for this genre\n",
    "            genre_words = Counter()\n",
    "            for word_counts in genre_data['word_counts']:\n",
    "                genre_words.update(word_counts)\n",
    "            \n",
    "            genre_stats[genre] = {\n",
    "                'data': genre_data,\n",
    "                'sentiment': genre_sentiment,\n",
    "                'common_words': genre_words\n",
    "            }\n",
    "        \n",
    "        return genre_stats\n",
    "    \n",
    "    def plot_sentiment_by_genre(self, genre_stats):\n",
    "        \"\"\"Plot comparison of sentiment across genres\"\"\"\n",
    "        if not genre_stats:\n",
    "            return None\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        genre_names = list(genre_stats.keys())\n",
    "        sentiment_means = {\n",
    "            'negative': [genre_stats[g]['sentiment']['negative'].mean() for g in genre_names],\n",
    "            'neutral': [genre_stats[g]['sentiment']['neutral'].mean() for g in genre_names],\n",
    "            'positive': [genre_stats[g]['sentiment']['positive'].mean() for g in genre_names],\n",
    "            'compound': [genre_stats[g]['sentiment']['compound'].mean() for g in genre_names]\n",
    "        }\n",
    "        \n",
    "        # Create bar plot\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        x = np.arange(len(genre_names))\n",
    "        width = 0.2\n",
    "        \n",
    "        ax.bar(x - width*1.5, sentiment_means['negative'], width, label='Negative', color='red', alpha=0.7)\n",
    "        ax.bar(x - width*0.5, sentiment_means['neutral'], width, label='Neutral', color='gray', alpha=0.7)\n",
    "        ax.bar(x + width*0.5, sentiment_means['positive'], width, label='Positive', color='green', alpha=0.7)\n",
    "        ax.bar(x + width*1.5, sentiment_means['compound'], width, label='Compound', color='blue', alpha=0.7)\n",
    "        \n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(genre_names, rotation=45, ha='right')\n",
    "        ax.set_ylabel('Mean Sentiment Score')\n",
    "        ax.set_title('Sentiment Analysis by Genre')\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return plt\n",
    "        \n",
    "    def run_full_analysis(self, genre_file='genre_assignment.txt', test_file='mxm_dataset_test.txt'):\n",
    "        \"\"\"Run a complete analysis pipeline on the dataset with genre prediction\"\"\"\n",
    "        # Load and merge data\n",
    "        start_merge_data = timer()\n",
    "        merged_data = self.merge_data()\n",
    "        end_merge_data = timer()\n",
    "        \n",
    "        # Create word count matrix\n",
    "        self.create_word_count_matrix()\n",
    "        \n",
    "        # Calculate sentiment\n",
    "        start_sentiment = timer()\n",
    "        sentiment_df = self.calculate_sentiment(merged_data)\n",
    "        end_sentiment = timer()\n",
    "        \n",
    "        # Load genre data\n",
    "        genre_data = self.load_genre_data(genre_file)\n",
    "        \n",
    "        # Merge genre data\n",
    "        merged_data_with_genre = self.merge_genre_data(genre_data, merged_data)\n",
    "        \n",
    "        # Prepare features for classification\n",
    "        remove_words = ['i', 'the', 'a', 'to', 'you', 'and', 'me']\n",
    "        X, vectorizer = self.prepare_features_for_classification(merged_data_with_genre, remove_words)\n",
    "        y = merged_data_with_genre['genre']\n",
    "\n",
    "        \n",
    "        # Split data for training and evaluation\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Train genre classifier\n",
    "        start_svm_train = timer()\n",
    "        model = self.train_genre_classifier(X_train, y_train)\n",
    "        end_svm_train = timer()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        accuracy, report, confusion_plot = self.evaluate_genre_classifier(model, X_val, y_val)\n",
    "        \n",
    "        # Load test data\n",
    "        test_df, test_words = self.load_test_data(test_file)\n",
    "        \n",
    "        # Predict genres for test data\n",
    "        predictions_df = self.predict_genres(model, vectorizer, test_df, genre_data, remove_words)\n",
    "        \n",
    "        # Generate plots\n",
    "        plots = {\n",
    "            #'common_words': self.plot_most_common_words(top_n=20),\n",
    "            'sentiment_distribution': self.plot_sentiment_distribution(sentiment_df),\n",
    "            'genre_visualization': self.plot_genre_visualization(merged_data_with_genre),\n",
    "            'confusion_matrix': confusion_plot\n",
    "        }\n",
    "\n",
    "        timers = {\n",
    "                'merge_data': [end_merge_data-start_merge_data],\n",
    "                'sentiment': [end_sentiment-start_sentiment],\n",
    "                'training': [end_svm_train-start_svm_train]\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'merged_data': merged_data,\n",
    "            'merged_data_with_genre': merged_data_with_genre,\n",
    "            'sentiment': sentiment_df,\n",
    "            'genre_model': model,\n",
    "            'vectorizer': vectorizer,\n",
    "            'validation_accuracy': accuracy,\n",
    "            'validation_report': report,\n",
    "            'test_predictions': predictions_df,\n",
    "            'plots': plots\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matches data...\n",
      "Loaded 779056 song matches.\n",
      "Loading lyrics data...\n",
      "Loaded lyrics data for 210519 songs with 5000 vocabulary words.\n",
      "Merged data contains 265353 songs.\n",
      "Loading genre data...\n",
      "Loaded genre data for 422714 songs.\n",
      "Merged data contains 137227 songs with genre information.\n",
      "Training SVM classifier for genre prediction...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the analyzer with file paths\n",
    "analyzer = LyricsAnalyzer('mxm_779k_matches.txt', 'mxm_dataset_train.txt')\n",
    "\n",
    "# Run the analysis\n",
    "results = analyzer.run_full_analysis()\n",
    "\n",
    "# Show plots\n",
    "#results['plots']['common_words'].show()\n",
    "results['plots']['sentiment_distribution'][0].show()\n",
    "#results['plots']['clusters'].show()\n",
    "\n",
    "# Example of comparing genres\n",
    "# genre_mapping = {\n",
    "#     'Rock': ['Queen', 'The Beatles', 'Led Zeppelin', 'Pink Floyd', 'Rolling Stones'],\n",
    "#     'Pop': ['Michael Jackson', 'Madonna', 'Prince'],\n",
    "#     'Rap': ['Eminem', 'Jay-Z', 'Tupac']\n",
    "# } \n",
    "\n",
    "# genre_stats = analyzer.compare_genres(genre_mapping)\n",
    "# analyzer.plot_sentiment_by_genre(genre_stats).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['timers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['merged_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['clusters'].describe(include='all')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of comparing genres\n",
    "genre_mapping = {\n",
    "    'ABBA': ['ABBA'],\n",
    "    'Nirvana': ['Nirvana'],\n",
    "    'Taylor Swift': ['Taylor Swift'],\n",
    "    'Michael Jackson': ['Michael Jackson'],\n",
    "    'Eminem': ['Eminem'],\n",
    "    'Bob Dylan': ['Bob Dylan'],\n",
    "    'Joy Division': ['Joy Division'],\n",
    "    'Radiohead': ['Radiohead'],\n",
    "    'The Smiths': ['The Smiths'],\n",
    "    'Lady Gaga': ['Lady Gaga'],\n",
    "    'Linkin Park': ['Linkin Park'],\n",
    "    'Travis Scott': ['Travis Scott'],\n",
    "    'Kendrick Lamar': ['Kendrick Lamar']\n",
    "}\n",
    "\n",
    "genre_stats = analyzer.compare_genres(genre_mapping)\n",
    "analyzer.plot_sentiment_by_genre(genre_stats).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNj96NopttSy"
   },
   "source": [
    "# Problem description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5g9ejSWtmd_"
   },
   "source": [
    "# Workbook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_rYZ3QktjOL"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzLMnofktx5N"
   },
   "source": [
    "# Analysis"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOm6kK/hV474xvJnYQeU9ph",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
