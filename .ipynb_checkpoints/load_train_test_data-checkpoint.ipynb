from pyspark.sql import SparkSession
from pyspark.sql.functions import col, split, explode
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType
import time

# Initialize SparkSession with optimized settings
spark = SparkSession.builder \
    .master("spark://192.168.2.205:7077") \
    .appName("LoadLargeLyricsDataToPostgres") \
    .config("spark.jars", "/home/ubuntu/postgresql-42.7.3.jar") \
    .config("spark.executor.instances", "4")      # Use all 4 workers \
    .config("spark.executor.cores", "4")          # 4 cores per worker \
    .config("spark.executor.memory", "8g")        # 8 GiB per worker \
    .config("spark.default.parallelism", "16")    # Match total cores \
    .config("spark.sql.shuffle.partitions", "16") # Parallelism for shuffles \
    .getOrCreate()

# Verify cluster setup
print("Cluster Info:")
print(spark.sparkContext._jsc.sc().getExecutorMemoryStatus())

# PostgreSQL connection properties
pg_url = "jdbc:postgresql://192.168.2.205:5432/Lyricsdb"
pg_properties = {
    "user": "sparkuser",
    "password": "9567",
    "driver": "org.postgresql.Driver",
    "batchsize": "10000"  # Faster writes for large data
}

# Process lyrics files (train/test)
def process_lyrics_file(file_path, is_test):
    start_time = time.time()
    
    raw_df = spark.read.text(file_path).repartition(16)
    top_words = raw_df.filter(col("value").startswith("%")) \
        .select(split(col("value").substr(2), ",").alias("words")) \
        .first()["words"]
    words_df = spark.createDataFrame(
        [(i + 1, word) for i, word in enumerate(top_words)],
        ["word_id", "word"]
    ).repartition(4)
    
    lyrics_raw = raw_df.filter(~col("value").startswith("#") & 
                              ~col("value").startswith("%") & 
                              (col("value") != ""))
    lyrics_split = lyrics_raw.select(
        split(col("value"), ",").alias("parts")
    ).filter(col("parts").getItem(0).isNotNull()) \
     .select(
        col("parts").getItem(0).alias("track_id"),
        col("parts").getItem(1).alias("mxm_tid"),
        col("parts").slice(2, 1000).alias("word_counts")
    )
    lyrics_exploded = lyrics_split.select(
        "track_id",
        "mxm_tid",
        explode(col("word_counts")).alias("word_count")
    ).select(
        "track_id",
        "mxm_tid",
        split(col("word_count"), ":").getItem(0).cast("int").alias("word_id"),
        split(col("word_count"), ":").getItem(1).cast("int").alias("count")
    ).withColumn("is_test", when(is_test, True).otherwise(False)) \
     .repartition(16)
    
    end_time = time.time()
    print(f"Processing {file_path} took {end_time - start_time:.2f} seconds")
    return words_df, lyrics_exploded

# Total timing start
total_start_time = time.time()

# Load train and test files
print("Loading train data...")
train_words_df, train_lyrics_df = process_lyrics_file("/home/ubuntu/DE1-G29/mxm_dataset_train.txt", False)
print("Loading test data...")
test_words_df, test_lyrics_df = process_lyrics_file("/home/ubuntu/DE1-G29/mxm_dataset_test.txt", True)

# Combine data
print("Combining data...")
start_time = time.time()
words_df = train_words_df.union(test_words_df).distinct().cache()
lyrics_df = train_lyrics_df.union(test_lyrics_df).repartition(16).cache()
end_time = time.time()
print(f"Combining took {end_time - start_time:.2f} seconds")

# Load matches file (custom format: tid|artist|title|mxm_tid|artist|title)
print("Loading matches data...")
start_time = time.time()
matches_raw = spark.read.text("/home/ubuntu/DE1-G29/mxm_779k_matches.txt") \
    .filter(~col("value").startswith("#") & (col("value") != "")) \
    .repartition(16)
matches_df = matches_raw.select(
    split(col("value"), "\\|").alias("parts")  # Escape | since itâ€™s a regex special char
).select(
    col("parts").getItem(0).alias("track_id"),
    col("parts").getItem(3).cast("int").alias("mxm_tid"),
    col("parts").getItem(1).alias("msd_artist_name"),
    col("parts").getItem(2).alias("msd_title"),
    col("parts").getItem(4).alias("mxm_artist_name"),
    col("parts").getItem(5).alias("mxm_title")
).repartition(16).cache()
end_time = time.time()
print(f"Loading matches took {end_time - start_time:.2f} seconds")

# Write to PostgreSQL
print("Writing to PostgreSQL...")
start_time = time.time()
words_df.write.jdbc(url=pg_url, table="words", mode="append", properties=pg_properties)
print(f"Words write took {time.time() - start_time:.2f} seconds")

start_time = time.time()
lyrics_df.write.jdbc(url=pg_url, table="lyrics", mode="append", properties=pg_properties)
print(f"Lyrics write took {time.time() - start_time:.2f} seconds")

start_time = time.time()
matches_df.write.jdbc(url=pg_url, table="matches", mode="append", properties=pg_properties)
print(f"Matches write took {time.time() - start_time:.2f} seconds")

# Total time
total_end_time = time.time()
print(f"Total execution time: {total_end_time - total_start_time:.2f} seconds")

# Stop SparkSession
spark.stop()
